{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f6a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34236cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences   \n",
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense, Input,Embedding ,LSTM, Bidirectional, Dropout, TimeDistributed ,GRU , Activation , RepeatVector \n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a33c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4276047656908852225\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f8256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = path\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')\n",
    "english_sentences = load_data('data/english')\n",
    "french_sentences = load_data('data/french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ce5306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new jersey is sometimes quiet during autumn , and it is snowy in april .',\n",
       " 'the united states is usually chilly during july , and it is usually freezing in november .',\n",
       " 'california is usually quiet during march , and it is usually hot in june .',\n",
       " 'the united states is sometimes mild during june , and it is cold in september .',\n",
       " 'your least liked fruit is the grape , but my least liked is the apple .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a8c7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique English words: 227\n",
      "Unique French words: 355\n"
     ]
    }
   ],
   "source": [
    "english_word_count = collections.Counter(word for sentence in english_sentences for word in sentence.split())\n",
    "french_word_count = collections.Counter(word for sentence in french_sentences for word in sentence.split())\n",
    "print(f\"Unique English words: {len(english_word_count)}\")\n",
    "print(f\"Unique French words: {len(french_word_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b0a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sequences: [[5, 6, 7], [8, 1, 9, 10, 11, 12], [3, 2, 1, 13], [14, 2, 1, 4, 15, 16, 3, 2], [17, 18, 19, 1, 4, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tok = Tokenizer()\n",
    "    tok.fit_on_texts(x)\n",
    "    return tok.texts_to_sequences(x), tok\n",
    "\n",
    "text_sentences = [\n",
    "    'I love programming',\n",
    "    'Python is great for data science',\n",
    "    'Machine learning is fascinating',\n",
    "    'Deep learning is a subset of machine learning',\n",
    "    'Natural language processing is a complex field'\n",
    "]\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenizer(text_sentences)\n",
    "print(\"Tokenized Sequences:\", text_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25e95cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      " [[ 5  6  7  0  0  0  0  0]\n",
      " [ 8  1  9 10 11 12  0  0]\n",
      " [ 3  2  1 13  0  0  0  0]\n",
      " [14  2  1  4 15 16  3  2]\n",
      " [17 18 19  1  4 20 21  0]]\n"
     ]
    }
   ],
   "source": [
    "def pad(x , length=None):\n",
    "    if length is None:\n",
    "        length = max(len(i) for i in x)\n",
    "    return pad_sequences(x , maxlen=length , padding='post')\n",
    "text_pad = pad(text_tokenized)\n",
    "print(\"Padded Sequences:\\n\", text_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ded8fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x,y):\n",
    "    preprocess_x, x_tk = tokenizer(x)\n",
    "    preprocess_y, y_tk = tokenizer(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794f09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
